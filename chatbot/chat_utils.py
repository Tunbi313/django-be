

import os
from dotenv import load_dotenv
import psycopg2
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader, TextLoader  # Th√™m UnstructuredWordDocumentLoader n·∫øu d√πng .docx
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings # ƒê·ªÉ t·∫°o embeddings c·ª•c b·ªô
from langchain_community.vectorstores import FAISS # Vector Store c·ª•c b·ªô
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage # C·∫ßn import ƒë·∫ßy ƒë·ªß c√°c lo·∫°i Message
from pydantic import SecretStr
from langchain.docstore.document import Document
from langchain.chains import create_retrieval_chain


# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng ngay khi module ƒë∆∞·ª£c import
load_dotenv()
openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
openrouter_model = os.getenv("OPENROUTER_MODEL")
print("OPENROUTER_MODEL:", openrouter_model)
print("OPENROUTER_API_KEY",openrouter_api_key)

# Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng v√† raise l·ªói n·∫øu kh√¥ng t√¨m th·∫•y
if openrouter_api_key is None or openrouter_model is None:
    raise EnvironmentError("OpenRouter API Key ho·∫∑c Model ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ki·ªÉm tra file .env.")

#l·∫•y c√°c bi·∫øn m√¥i tr∆∞·ªùng cho database
db_host = os.getenv("DB_HOST")
db_port = os.getenv("DB_PORT")
db_name = os.getenv("DB_NAME")
db_user = os.getenv("DB_USER")
db_password = os.getenv("DB_PASSWORD")

# Kh·ªüi t·∫°o LLM (bi·∫øn to√†n c·ª•c, ƒë∆∞·ª£c kh·ªüi t·∫°o m·ªôt l·∫ßn)
llm = ChatOpenAI(
    model=openrouter_model,  # S·ª≠ d·ª•ng 'model' thay v√¨ 'model_name'
    temperature=0.3, 
    api_key=SecretStr(openrouter_api_key),  # B·ªçc api_key b·∫±ng SecretStr
    base_url="https://openrouter.ai/api/v1",
    
)

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ knowledge base v√† retrieval chain
knowledge_base = None
retrieval_chain = None
is_rag_ready = False # C·ªù b√°o hi·ªáu RAG ƒë√£ s·∫µn s√†ng hay ch∆∞a

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ chat (bao g·ªìm c·∫£ AI v√† user)
global_chat_history = []

def fetch_data_from_database(user_id=None):
    """
    K·∫øt n·ªëi database v√† tr√≠ch xu·∫•t d·ªØ li·ªáu d∆∞·ªõi d·∫°ng Document
    """
    db_documents = []
    if not all([db_host, db_name, db_user, db_password, db_port]):
        print("C·∫£nh b√°o :Sai c·∫•u h√¨nh Database trong .env")
        return db_documents  # Tho√°t h√†m n·∫øu thi·∫øu c·∫•u h√¨nh

    conn = None
    try: 
        conn = psycopg2.connect(
            host=db_host,
            database=db_name,
            user=db_user,
            password=db_password,
            port=db_port
        )
        cur = conn.cursor()  # S·ª≠a ·ªü ƒë√¢y

        sql_query = """
        SELECT p.id, p.name, p.description, p.price, p.created_at, c.name as category,
               CASE 
                   WHEN sp.discount_percent IS NOT NULL 
                   AND sp.start_date <= NOW() 
                   AND sp.end_date >= NOW() 
                   THEN p.price * (100 - sp.discount_percent) / 100
                   ELSE p.price 
               END as current_price,
               COALESCE(sp.discount_percent, 0) as discount_percent
        FROM products_product p
        LEFT JOIN products_category c ON p.category_id = c.id
        LEFT JOIN saleproduct_saleproduct sp ON p.id = sp.product_id;
        """
        cur.execute(sql_query)
        rows = cur.fetchall()

        for row in rows:
            product_id, name, description, price, created_at, category, current_price, discount_percent = row
            content = (
                
                f"T√™n: {name}.\n"
                f"M√¥ t·∫£: {description}.\n"
                f"Gi√° hi·ªán t·∫°i: {current_price}.\n"
                f"Gi·∫£m gi√°: {discount_percent}%.\n"
                f"Danh m·ª•c: {category}.\n"
                
            )
            metadata = {"source": "database", "table": "products", "name": name}
            db_documents.append(Document(page_content=content, metadata=metadata))

       


        cur.close()
        print(f"ƒê√£ t·∫£i th√†nh c√¥ng {len(db_documents)} document t·ª´ database.")
        if db_documents:
            print("D·ªØ li·ªáu ƒë·∫ßu ti√™n:", db_documents[0].page_content)
        else:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c ƒë·ªçc t·ª´ database.")

    except Exception as e:
        print(f"L·ªói khi k·∫øt n·ªëi ho·∫∑c truy v·∫•n database: {e}")
    finally:
        if conn:
            conn.close()
    return db_documents






def prepare_knowledge_base_sync(data_dir="data"):
    """
    T·∫£i t√†i li·ªáu, t·∫°o vector store (kh√¥ng split nh·ªè h∆°n t·ª´ng s·∫£n ph·∫©m).
    S·∫Ω ƒë∆∞·ª£c g·ªçi m·ªôt l·∫ßn khi server kh·ªüi ƒë·ªông.
    """
    global knowledge_base, retrieval_chain, is_rag_ready

    documents = []
    # Ki·ªÉm tra xem th∆∞ m·ª•c d·ªØ li·ªáu c√≥ t·ªìn t·∫°i kh√¥ng
    if os.path.exists(data_dir) and os.path.isdir(data_dir):
        print(f"ƒêang qu√©t th∆∞ m·ª•c d·ªØ li·ªáu: {data_dir}")
        for filename in os.listdir(data_dir):
            filepath = os.path.join(data_dir, filename)
            if os.path.isfile(filepath):
                if filename.endswith(".pdf"):
                    print(f"ƒêang t·∫£i file PDF: {filename}")
                    loader = PyPDFLoader(filepath)
                    documents.extend(loader.load())
                elif filename.endswith(".txt"):
                    print(f"ƒêang t·∫£i file TXT: {filename}")
                    loader = TextLoader(filepath, encoding="utf-8")
                    documents.extend(loader.load())
    else:
        print(f"Th∆∞ m·ª•c d·ªØ li·ªáu '{data_dir}' kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng ph·∫£i l√† th∆∞ m·ª•c. RAG s·∫Ω kh√¥ng ƒë∆∞·ª£c k√≠ch ho·∫°t.")
        is_rag_ready = False
        return

    # Th√™m: Load t·ª´ database (m·ªói s·∫£n ph·∫©m l√† 1 Document)
    db_documents = fetch_data_from_database()
    documents.extend(db_documents)


    if not documents:
        print("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o trong th∆∞ m·ª•c ho·∫∑c database.")
        is_rag_ready = False
        return

    # Kh√¥ng split nh·ªè h∆°n n·ªØa, m·ªói document l√† 1 s·∫£n ph·∫©m
    chunks = documents
    print(f"T·ªïng s·ªë document/chunk: {len(chunks)}")

    # In ra c√°c chunk ƒë·∫ßu ti√™n (t·ªëi ƒëa 10 chunk ƒë·∫ßu, 500 k√Ω t·ª± ƒë·∫ßu)
    
    print("ƒêang t·∫°o embeddings...")
    embeddings = SentenceTransformerEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")
    knowledge_base = FAISS.from_documents(chunks, embeddings)
    print("ƒê√£ t·∫°o Vector Store th√†nh c√¥ng.")

    # --- X√¢y d·ª±ng Chain cho RAG ---
    retriever = knowledge_base.as_retriever(search_kwargs={"k": 15})

    # Prompt t·ªëi ∆∞u - d√πng {context} ƒë√∫ng chu·∫©n RAG chain
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "B·∫°n l√† tr·ª£ l√Ω AI v·ªÅ s·∫£n ph·∫©m, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p,kh√¥ng ƒë∆∞·ª£c b·ªãa th√¥ng tin. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i r√µ."
             "N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ s·∫£n ph·∫©m h√£y tr·∫£ l·ªùi t·∫≠p trung v√†o th√¥ng tin s·∫£n ph·∫©m ·∫•y c√≥ c√≤n h√†ng hay kh√¥ng, gi√° c·∫£, m√¥ t·∫£, gi·∫£m gi√° (n·∫øu c√≥).Kh√¥ng tr·∫£ l·ªùi th√™m c√°c s·∫£n ph·∫©m kh√°c. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ƒë·∫ßy ƒë·ªß v√† ch√≠nh x√°c."
             "N·∫øu ng∆∞·ªùi d√πng h·ªèi m√† kh√¥ng c√≥ th√¥ng tin s·∫£n ph·∫©m th√¨ h√£y tr·∫£ l·ªùi l√† kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v√† g·ª£i √Ω cho ng∆∞·ªùi d√πng 1 s·∫£n ph·∫©m c√πng category nh∆∞ng ch·ªâ cung c·∫•p t√™n v√† gi√° ti·ªÅn ch·ª© kh√¥ng mi√™u t·∫£ th√™m g√¨ c·∫£."
             "N·∫øu ng∆∞·ªùi d√πng ƒë·ªìng √Ω v·ªÅ vi·ªác xem s·∫£n ph·∫©m kh√°c th√¨ kh√¥ng nh·∫Øc l·∫°i c√°c s·∫£n ph·∫©m ƒë√£ nh·∫Øc ·ªü tr√™n n≈©a m√† h√£y g·ª£i √Ω cho ng∆∞·ªùi d√πng 1-2 s·∫£n ph·∫©m kh√°c c√πng category v·ªõi s·∫£n ph·∫©m ƒë√£ h·ªèi."
             "N·∫øu ng∆∞·ªùi d√πng ch·ªâ h·ªèi v·ªÅ m·ªôt ph·∫©n c·ªßa s·∫£n ph·∫©m th√¨ t·∫≠p trung v√†o vi·ªác tr·∫£ l·ªùi cho ph·∫ßn ƒë√≥ c√≥ th·ªÉ h·ªèi ng∆∞·ªùi dung c√≥ mu·ªën xem chi ti·∫øt ph·∫ßn ƒë√≥ kh√¥ng "
             ),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{context}\n\nC√¢u h·ªèi: {input}\n\nCh·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu tr√™n.")
        ]
    )

    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    is_rag_ready = True
    print("ƒê√£ kh·ªüi t·∫°o Retrieval Chain.")


def rewrite_user_question(user_message, chat_history_parsed, max_turns=3):
    """
    D√πng LLM ƒë·ªÉ rewrite l·∫°i c√¢u h·ªèi user d·ª±a tr√™n N l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t (c·∫£ AI v√† user).
    In ra ƒëo·∫°n h·ªôi tho·∫°i g·∫ßn nh·∫•t v√† c√¢u h·ªèi ƒë√£ rewrite.
    """
    #Debug : s·ªë tin nh·∫Øn v√† ph√¢n lo·∫°i
    total_messages = len(chat_history_parsed)
    ai_messages = [msg for msg in chat_history_parsed if isinstance(msg, AIMessage)]
    user_messages = [msg for msg in chat_history_parsed if isinstance(msg, HumanMessage)]
    print(f"DEBUG - T·ªïng tin nh·∫Øn: {total_messages}, AI: {len(ai_messages)}, User: {len(user_messages)}")
    
    # L·∫•y max_turns l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t (m·ªói l∆∞·ª£t g·ªìm user v√† AI)
    # ƒê·∫£m b·∫£o lu√¥n c√≥ c·∫£ AI v√† user
    filtered = []
    ai_count = 0
    user_count = 0
    # Duy·ªát t·ª´ cu·ªëi l√™n ƒë·ªÉ l·∫•y tin nh·∫Øn g·∫ßn nh·∫•t
    for msg in reversed(chat_history_parsed):
        if isinstance(msg, (HumanMessage, AIMessage)):
            filtered.append(msg)
            if isinstance(msg, AIMessage):
                ai_count += 1
            elif isinstance(msg, HumanMessage):
                user_count += 1
            # D·ª´ng khi ƒë·ªß s·ªë l∆∞·ª£ng ho·∫∑c khi c√≥ ƒë·ªß c·∫£ AI v√† user
            if ai_count >= max_turns and user_count >= max_turns:
                break
    filtered.reverse()  # ƒê·∫£o l·∫°i th·ª© t·ª± ƒë·ªÉ gi·ªØ ƒë√∫ng th·ª© t·ª± th·ªùi gian
    
    print(f"DEBUG - Tin nh·∫Øn ƒë∆∞·ª£c l·ªçc: {len(filtered)}, AI: {ai_count}, User: {user_count}")
    
    # T·∫°o history_text ng·∫Øn g·ªçn
    history_lines = []
    for msg in filtered:
        role = "User" if isinstance(msg, HumanMessage) else "AI"
        history_lines.append(f"{role}: {msg.content}")
    history_text = "\n".join(history_lines)
    
    # Ki·ªÉm tra xem c√≥ tin nh·∫Øn AI kh√¥ng
    if ai_count == 0:
        print("WARNING: Kh√¥ng t√¨m th·∫•y tin nh·∫Øn AI trong history!")
        # N·∫øu kh√¥ng c√≥ AI, th√™m m·ªôt prompt ƒë·∫∑c bi·ªát
        prompt = (
            "Ng∆∞·ªùi d√πng ch·ªâ tr·∫£ l·ªùi ng·∫Øn g·ªçn. D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥, h√£y di·ªÖn gi·∫£i l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi r√µ r√†ng, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh.\n"
            f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{history_text}"
            f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {user_message}\n"
            "C√¢u h·ªèi ƒë√£ di·ªÖn gi·∫£i:"
        )
    else:
        prompt = (
            "D·ª±a v√†o ƒëo·∫°n h·ªôi tho·∫°i g·∫ßn nh·∫•t gi·ªØa ng∆∞·ªùi d√πng v√† AI d∆∞·ªõi ƒë√¢y, h√£y di·ªÖn gi·∫£i l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi r√µ r√†ng, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh ƒë·ªÉ AI c√≥ th·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c.\n"
            f"ƒêo·∫°n h·ªôi tho·∫°i g·∫ßn nh·∫•t:\n{history_text}"
            f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {user_message}\n"
            "C√¢u h·ªèi ƒë√£ di·ªÖn gi·∫£i:"
        )
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        rewritten = (response.content or user_message).strip()
    except Exception as e:
        print(f"‚ùå Rewrite l·ªói: {e}")
        return user_message

    # In debug
    print("---- ƒêo·∫°n h·ªôi tho·∫°i g·∫ßn nh·∫•t ----")
    print(history_text)
    print("---- C√¢u h·ªèi ƒë√£ rewrite ----")
    print(rewritten)

    return rewritten

def context_to_str(context):
    if isinstance(context, list):
        return "\n\n".join([getattr(doc, 'page_content', str(doc)) for doc in context])
    return str(context)

def context_to_data(context):
    return context_to_str(context)


def get_chatbot_response(user_message: str, chat_history_raw: list, user_id: int = None):
    """
    X·ª≠ l√Ω tin nh·∫Øn ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ ph·∫£n h·ªìi t·ª´ chatbot.
    """
    user_input_lower = user_message.lower()

    if any(keyword in user_input_lower for keyword in ["ƒë·ªïi h√†ng", "tr·∫£ h√†ng", "h·ªßy ƒë∆°n", "ƒë·ªïi tr·∫£", "ƒë·ªïi s·∫£n ph·∫©m","b·∫£o h√†nh"]):
        return {
            "answer": (
                "üîÑ Ch√≠nh s√°ch ƒë·ªïi/tr·∫£ h√†ng v√† b·∫£o h√†nh c·∫ßn x√°c nh·∫≠n qua c√°c h√¨nh th·ª©c sau.\n"
                "üìû Hotline: 0909 123 456    \n"
                "üí¨ Zalo: https://zalo.me/yourshop\n"
                "üí¨ Facebook: https://facebook.com/yourshop\n"
                
            )
        }


    
    global global_chat_history
    
    # K·∫øt h·ª£p l·ªãch s·ª≠ t·ª´ frontend v·ªõi l·ªãch s·ª≠ to√†n c·ª•c
    combined_history = global_chat_history.copy()
    
    # Th√™m tin nh·∫Øn user m·ªõi v√†o l·ªãch s·ª≠
    combined_history.append({"role": "human", "content": user_message})
    
    chat_history_parsed = []
    for msg in combined_history:
        if isinstance(msg, dict):
            if msg.get("role") == "human":
                chat_history_parsed.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") == "ai":
                chat_history_parsed.append(AIMessage(content=msg.get("content", "")))
            elif msg.get("role") == "system":
                chat_history_parsed.append(SystemMessage(content=msg.get("content", "")))
        elif isinstance(msg, str):
            chat_history_parsed.append(HumanMessage(content=msg))

    try:
        # Rewrite l·∫°i c√¢u h·ªèi user d·ª±a v√†o ng·ªØ c·∫£nh h·ªôi tho·∫°i (AI + user)
        user_message_rewritten = rewrite_user_question(user_message, chat_history_parsed)
        if is_rag_ready and retrieval_chain:
            print("S·ª≠ d·ª•ng RAG chain...")
            response = retrieval_chain.invoke({
                "input": user_message_rewritten,
                "history": chat_history_parsed
            })
            # ƒê·∫£m b·∫£o context l√† string
            context_str = context_to_str(response["context"]) if "context" in response else ""
            print("DEBUG - Context truy·ªÅn v√†o prompt:")
            print(context_str)
            print(f"DEBUG - Input g·ª≠i ƒë·∫øn LLM: {user_message_rewritten}")
            # Render prompt th·ª±c t·∫ø g·ª≠i ƒë·∫øn LLM
            prompt_text = f"{context_str}\n\nC√¢u h·ªèi: {user_message_rewritten}\n\nCh·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu tr√™n."
            print("DEBUG - Prompt th·ª±c t·∫ø g·ª≠i ƒë·∫øn LLM:")
            print(prompt_text)
            ai_response = response["answer"]
            print(f"DEBUG - AI response t·ª´ RAG: '{ai_response}'")
        else:
            print("S·ª≠ d·ª•ng LLM c∆° b·∫£n (kh√¥ng c√≥ RAG).")
            if not chat_history_parsed or chat_history_parsed[0].type != "system":
                chat_history_parsed.insert(0, SystemMessage(content="B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch v√† th√¢n thi·ªán. B·∫°n s·∫Ω tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."))
            chat_history_parsed.append(HumanMessage(content=user_message_rewritten))
            llm_response = llm.invoke(chat_history_parsed)
            ai_response = llm_response.content
        
        # L∆∞u tin nh·∫Øn AI v√†o l·ªãch s·ª≠ to√†n c·ª•c
        global_chat_history.append({"role": "ai", "content": ai_response})
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i l·ªãch s·ª≠ ƒë·ªÉ tr√°nh qu√° t·∫£i (gi·ªØ t·ªëi ƒëa 20 tin nh·∫Øn)
        if len(global_chat_history) > 20:
            global_chat_history = global_chat_history[-20:]
        
        return {"answer": ai_response}
    except Exception as e:
        print(f"L·ªói LangChain/LLM khi x·ª≠ l√Ω chat: {e}")
        return {"answer": "Xin l·ªói, t√¥i ƒëang g·∫∑p v·∫•n ƒë·ªÅ n·ªôi b·ªô. Vui l√≤ng th·ª≠ l·∫°i sau."}